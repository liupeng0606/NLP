# <center>NLP入门总结</center>

## **part 1 for beginners bag of words**
1. **数据读取**<br>
`train = pd.read_csv("labeledTrainData.tsv", header=0, \
        delimiter="\t", quoting=3)
`<br>
`>>> train.shape `<br>
`(25000, 3)`<br>
数据有25000行,三列.

2.  **数据清洗和文本处理**<br>
由于数据中有html标签,引入BeautifulSoup库进行清理,同时使用正则表达式,把标点符号和数字替换为空格,然后出去所有的大写字母,方便后来的单词处理  (***例如Love和love,实质是一个单词,如果不把大小字母处理,可能会识别为两个单词***),还要去除 **stop words**,因为 stop words 出现的频率高,但是意义不大.关键代码如下：<br>
`example1 = BeautifulSoup(train["review"][0]).get_text()`<br>
`letters_only = re.sub("[^a-zA-Z]", " ")`<br>
`lower_case = letters_only.lower() `<br>
`words = lower_case.split()  `<br>
然后把每一个review的处理后的词，放入list中
3. **使用词袋模型创建特征向量**<br>
Sentence 1: "The cat sat on the hat"<br>
Sentence 2: "The dog ate the cat and the hat"<br>
From these two sentences, our vocabulary is as follows:<br>
M = { the, cat, sat, on, hat, dog, ate, and }<br>
集合M称为词汇表，词汇表是从所有的训练文本中提取出来的词汇的集合<br>
Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }<br>
Sentence 2: { 3, 1, 0, 0, 1, 1, 1, 1}<br>
把每一个Sentence转化为一个向量<br>
***词汇表的长度n，就是每一个Sentence向量的维度***<br>
这就是使用词袋模型创建特征向量的基本方法<br>
此时统计review样本中的词汇，找到词汇表M，然后按照上述方法，将每一个review转化为特征向量（**特征向量的维度取决于词汇表的长度n**）

4. **随机森林算法**<br>
有了每一个review的特征向量，还有每一个review的sentiment,采用随机深林算法<br>
`forest = forest.fit( train_data_features, train["sentiment"] )`<br>
算法完成以后，可以做预测<br>
`result = forest.predict(test_data_features)`<br>
其中test_data_features无sentiment标签<br>
`output = pd.DataFrame(data={"id":test["id"], "sentiment":result})`
___
## **part 2 word vectors**

# 问题引入
>## 什么是one-hot编码，该编码有什么缺点？
任何一门语言，都是由一堆的词组成，所有的词，构成了一个词汇表。词汇表，可以用一个长长的向量来表示。词的个数，就是词汇表向量的维度。那么，任何一个词，都可以表示成一个向量，词在词汇表中出现的位置设为1，其它的位置设为0。但是这种词向量的表示，词和词之间没有交集，用处不大。

>## 为什么需要word2vec，word2vec主要是干什么用的？
为了克服one-hot编码的缺点,word2vec利用上下文信息推测词的意思，这种方法训练出来的词，相似的词，词的特征向量之间的夹角越小。<br><br>
Word2Vec 的训练模型，是具有一个隐含层的神经元网络，有两种训练方法
CBOW (Continuous Bag-of-Words Model) 和 Skip-gram (Continuous Skip-gram Model)<br><br>

CBOW，它的做法是，将一个词所在的上下文中的词作为输入，而那个词本身作为输出，也就是说，看到一个上下文，希望大概能猜出这个词和它的意思。<br><br>
Skip-gram，它的做法是，将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词
___
# part2正式内容
1. **word2vec介绍**<br>
word2vec由谷歌在2013年发布,balabala...............

2. **如何在python中使用**<br>
如何安装，此处省略。。。。。。。。

3. **训练模型，保存训练后的模型，测试**<br>
主要代码如下：<br>
`model = word2vec.Word2Vec(sentences, workers=num_workers, \`
            `size=num_features, min_count = min_word_count, \`
            `window = context, sample = downsampling)`<br><br>
`Word2Vec.load()`<br>
`model_name = "300features_40minwords_10context"`<br>
`model.save(model_name)`<br>
测试模型<br>
` >>model.doesnt_match("man woman child kitchen".split())`<br>
`'kitchen'`<br>
```
model.most_similar("man")
[(u'woman', 0.6056041121482849), (u'guy', 0.4935004413127899), (u'boy', 0.48933547735214233), (u'men', 0.4632953703403473), (u'person', 0.45742249488830566), (u'lady', 0.4487500488758087), (u'himself', 0.4288588762283325), (u'girl', 0.4166809320449829), (u'his', 0.3853422999382019), (u'he', 0.38293731212615967)]
```
## **part 3 more fun with word vectors**
1. **词语的特征向量**<br>
由上一个部分的训练可以知道<br>
`>>> model["flower"]`<br>
输出是一个 1x300 的向量

2. **从单词到句子，尝试使用平均向量**<br>
有一个很大的问题需要想办法解决，训练数据集中的每一个review包含的单词数量不一，需要找到一种方法，把每一个review都映射成一个长度相同的特征向量。有一个含简单的办法就是，取review中词的均值向量（**把review中的所有向量相加，然后除以数量，得到平均向量**），然后把review的这个平均向量和review的sentiment输入到随机森林算法，然后测试预测数据<br>
<font color=#ff0000>***但是这种方法，正确率表现平平***</font>

3. **再次尝试，使用聚类方法**<br>
把 part2 训练好的词向量，做聚类，group的平均大小，根据前人的经验得知，过大过小都不好，5,6个最为合适，就采用5个；<br>
聚类之后，大概是这个样子<br>
```
Cluster 0
[u'passport', u'penthouse', u'suite', u'seattle', u'apple']

Cluster 1
[u'unnoticed']

Cluster 2
[u'midst', u'forming', u'forefront', u'feud', u'bonds', u'merge', u'collide', u'dispute', u'rivalry', u'hostile', u'torn', u'advancing', u'aftermath', u'clans', u'ongoing', u'paths', u'opposing', u'sexes', u'factions', u'journeys']
```

此时我们再结合词袋模型,生成每一个review的特征向量，采用随机森林算法
这样的模型，正确率比较好

<font color=#00ff00>***补充***</font><br>
每一个review特征向量的维度，等同于簇（类）的数量


<font color=#0000ff size="10">***小结***</font><br>
part 1 的代码在github<br>
https://github.com/liupeng0606/comp/blob/master/Test.py

part 2,由于电脑性能问题，没有训练数据，代码没有实现

part 3，该部分利用part 2的训练模型，代码没有实现

